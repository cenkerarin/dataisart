# Data Collection Guide

This guide explains how to collect real gesture data to improve your hand tracking model.

## Quick Start

### 1. Test the System
```bash
cd src/core/hand_tracking
python test_data_collection.py
```

### 2. Collect Real Gesture Data
```bash
python data_collector.py
```

### 3. Test with Real-Time Recognition
```bash
python gesture_classification_demo.py
```

## What the Data Collector Does

- **Interactive Collection**: Uses your camera to capture real gesture samples
- **Visual Feedback**: Shows hand landmarks and bounding boxes in real-time
- **Quality Control**: Only captures samples when hand is properly detected
- **Structured Storage**: Saves data in JSON format for training

## Collection Process

The collector will guide you through capturing 7 different gestures:

1. **Neutral** - Relaxed hand
2. **Pointing** - Index finger extended
3. **Fist** - Closed fist
4. **Open Hand** - All fingers extended
5. **Peace Sign** - Index and middle fingers extended
6. **Thumbs Up** - Thumb extended
7. **Pinch** - Thumb and index close together

For each gesture, you'll collect **40 samples** by pressing SPACE when your hand is in position.

## Controls

- **SPACE** - Capture a sample
- **Q** - Quit early / Skip to next gesture
- **Any Key** - Start collection for current gesture

## Tips for Better Data Collection

### ðŸŽ¯ **Variety is Key**
- Change hand position (left, right, center)
- Vary distance from camera (near, far)
- Try different angles and rotations
- Use both hands if comfortable

### ðŸ’¡ **Lighting**
- Ensure good, even lighting
- Avoid shadows on your hand
- Natural light works best

### ðŸ¤š **Gesture Quality**
- Hold pose steady when capturing
- Make gestures clear and distinct
- Be consistent with gesture definition

### ðŸ“Š **Data Quality**
- Collect samples over multiple sessions
- Different people can contribute samples
- More variation = better generalization

## Output Files

The collector creates:
- `./gesture_data/training_data.json` - Your collected gesture samples
- Automatic backup with timestamp

## Data Format

```json
{
  "statistics": {
    "total_samples": 280,
    "gestures": {
      "pointing": 40,
      "fist": 40,
      ...
    }
  },
  "data": {
    "pointing": [
      {
        "gesture": "pointing",
        "landmarks": [...],  // 21 hand landmarks
        "timestamp": 1234567890,
        "sample_id": 0
      }
    ]
  }
}
```

## Training with Your Data

Once you have collected data, the gesture classifier will automatically:
1. Load your real data instead of synthetic data
2. Train with better accuracy
3. Provide improved real-time recognition

## Performance Expectations

With good quality real data:
- **Accuracy**: 85-95% (vs 70-80% with synthetic)
- **Robustness**: Works across different users and conditions
- **Real-time**: 30+ FPS performance maintained

## Troubleshooting

### Camera Issues
- Check camera permissions
- Ensure no other apps are using camera
- Try different camera index if needed

### Hand Detection Issues
- Improve lighting conditions
- Ensure hand is clearly visible
- Adjust detection confidence in collector

### Low Sample Count
- Minimum 20 samples per gesture recommended
- More samples = better performance
- Quality over quantity

## Next Steps

After collecting data:
1. Run `python gesture_classification_demo.py` to test real-time recognition
2. Experiment with different gestures by modifying the collector
3. Share data with team members for better model generalization

## Advanced Usage

### Custom Gestures
Edit the `gestures` list in `data_collector.py` to add your own gestures:

```python
gestures = [
    "neutral",
    "pointing", 
    "your_custom_gesture",
    # Add more gestures here
]
```

### Batch Collection
You can collect data in multiple sessions and it will accumulate in the same file.

### Data Analysis
Use the statistics in the JSON file to analyze your data quality and distribution.

---

**Happy Data Collecting! ðŸš€** 