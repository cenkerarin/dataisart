# ğŸ“ Project Structure Documentation

## Overview
This document provides a comprehensive guide to the **Hands-Free Data Science** project structure, explaining the purpose and organization of each directory and file.

## ğŸ“‚ Directory Structure

```
dataisart/
â”œâ”€â”€ ğŸ“‹ README.md                          # Main project documentation
â”œâ”€â”€ ğŸ“„ PROJECT_STRUCTURE.md               # This file - project structure guide
â”œâ”€â”€ ğŸš€ main.py                           # Main application entry point
â”œâ”€â”€ ğŸ“¦ requirements.txt                   # Python dependencies
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“ src/                              # Source code directory
â”‚   â”œâ”€â”€ ğŸ“ core/                         # Core functionality modules
â”‚   â”‚   â”œâ”€â”€ ğŸ“ hand_tracking/            # Hand gesture recognition
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ–ï¸ gesture_detector.py   # MediaPipe hand tracking
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“Š gesture_classifier.py # Gesture classification logic
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ¯ selection_handler.py  # Data selection from gestures
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ ğŸ“ voice_recognition/        # Voice command processing
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ¤ voice_handler.py      # Speech-to-text conversion
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ§  command_parser.py     # Command intent recognition
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ™ï¸ audio_processor.py    # Audio preprocessing
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ ğŸ“ llm_integration/          # AI assistant functionality
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ¤– ai_assistant.py       # OpenAI GPT integration
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ’¬ prompt_manager.py     # Prompt engineering
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ”„ response_parser.py    # AI response processing
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ ğŸ“ data_processing/          # Data manipulation
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“Š data_manager.py       # Dataset loading & management
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ” data_analyzer.py      # Statistical analysis
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ§¹ data_cleaner.py       # Data preprocessing
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ ğŸ“ visualization/            # Chart generation
â”‚   â”‚       â”œâ”€â”€ ğŸ“ˆ chart_generator.py    # Plotly visualizations
â”‚   â”‚       â”œâ”€â”€ ğŸ¨ style_manager.py      # Chart styling
â”‚   â”‚       â””â”€â”€ ğŸ“Š plot_templates.py     # Pre-built chart templates
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ ui/                           # User interface components
â”‚   â”‚   â”œâ”€â”€ ğŸ“ pages/                    # Streamlit pages
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ  main_dashboard.py     # Main application dashboard
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“Š data_explorer.py      # Data exploration page
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ¯ gesture_training.py   # Gesture training interface
â”‚   â”‚   â”‚   â””â”€â”€ âš™ï¸ settings.py           # Application settings
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ ğŸ“ components/               # Reusable UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“¹ camera_widget.py      # Camera display widget
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ¤ voice_widget.py       # Voice input widget
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“Š data_table.py         # Enhanced data table
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“ˆ chart_widget.py       # Chart display widget
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ ğŸ“ styles/                   # UI styling
â”‚   â”‚       â”œâ”€â”€ ğŸ¨ app_styles.css        # Main application styles
â”‚   â”‚       â””â”€â”€ ğŸ“± mobile_styles.css     # Mobile-responsive styles
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ utils/                        # Utility functions
â”‚       â”œâ”€â”€ ğŸ“ helpers/                  # Helper functions
â”‚       â”‚   â”œâ”€â”€ âš™ï¸ config_loader.py      # Configuration management
â”‚       â”‚   â”œâ”€â”€ ğŸ“ logger.py             # Logging utilities
â”‚       â”‚   â””â”€â”€ ğŸ”§ file_utils.py         # File handling utilities
â”‚       â”‚
â”‚       â””â”€â”€ ğŸ“ validators/               # Input validation
â”‚           â”œâ”€â”€ ğŸ“Š data_validators.py    # Dataset validation
â”‚           â”œâ”€â”€ ğŸ¤ voice_validators.py   # Voice command validation
â”‚           â””â”€â”€ ğŸ–ï¸ gesture_validators.py # Gesture validation
â”‚
â”œâ”€â”€ ğŸ“ config/                           # Configuration files
â”‚   â”œâ”€â”€ ğŸ“ app/                          # Application configuration
â”‚   â”‚   â”œâ”€â”€ âš™ï¸ settings.py               # Main app settings
â”‚   â”‚   â””â”€â”€ ğŸŒ environment.py            # Environment variables
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ models/                       # Model configurations
â”‚   â”‚   â”œâ”€â”€ ğŸ¤– llm_config.py             # LLM model settings
â”‚   â”‚   â”œâ”€â”€ ğŸ–ï¸ gesture_config.py        # Hand tracking settings
â”‚   â”‚   â””â”€â”€ ğŸ¤ voice_config.py           # Voice recognition settings
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ api/                          # API configurations
â”‚       â”œâ”€â”€ ğŸ”‘ openai_config.py          # OpenAI API settings
â”‚       â””â”€â”€ ğŸŒ endpoints.py              # API endpoints
â”‚
â”œâ”€â”€ ğŸ“ data/                             # Data storage
â”‚   â”œâ”€â”€ ğŸ“ raw/                          # Raw uploaded datasets
â”‚   â”œâ”€â”€ ğŸ“ processed/                    # Processed datasets
â”‚   â””â”€â”€ ğŸ“ sample/                       # Sample datasets for testing
â”‚       â”œâ”€â”€ ğŸŒº iris.csv                  # Iris dataset
â”‚       â”œâ”€â”€ ğŸš¢ titanic.csv               # Titanic dataset
â”‚       â””â”€â”€ ğŸ  housing.csv               # Boston housing dataset
â”‚
â”œâ”€â”€ ğŸ“ tests/                            # Test files
â”‚   â”œâ”€â”€ ğŸ“ unit/                         # Unit tests
â”‚   â”‚   â”œâ”€â”€ ğŸ§ª test_data_manager.py      # Data manager tests
â”‚   â”‚   â”œâ”€â”€ ğŸ§ª test_gesture_detector.py  # Gesture detection tests
â”‚   â”‚   â”œâ”€â”€ ğŸ§ª test_voice_handler.py     # Voice recognition tests
â”‚   â”‚   â””â”€â”€ ğŸ§ª test_ai_assistant.py      # AI assistant tests
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ integration/                  # Integration tests
â”‚   â”‚   â”œâ”€â”€ ğŸ”— test_voice_to_ai.py       # Voice-to-AI integration
â”‚   â”‚   â”œâ”€â”€ ğŸ”— test_gesture_to_data.py   # Gesture-to-data integration
â”‚   â”‚   â””â”€â”€ ğŸ”— test_full_pipeline.py     # End-to-end tests
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ core/                         # Core module tests
â”‚   â”œâ”€â”€ ğŸ“ ui/                           # UI component tests
â”‚   â””â”€â”€ ğŸ“ utils/                        # Utility function tests
â”‚
â”œâ”€â”€ ğŸ“ docs/                             # Documentation
â”‚   â”œâ”€â”€ ğŸ“ api/                          # API documentation
â”‚   â”‚   â”œâ”€â”€ ğŸ“– core_modules.md           # Core module APIs
â”‚   â”‚   â””â”€â”€ ğŸ“– ui_components.md          # UI component APIs
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ user_guide/                   # User documentation
â”‚   â”‚   â”œâ”€â”€ ğŸ“˜ getting_started.md        # Getting started guide
â”‚   â”‚   â”œâ”€â”€ ğŸ“˜ voice_commands.md         # Voice command reference
â”‚   â”‚   â”œâ”€â”€ ğŸ“˜ hand_gestures.md          # Hand gesture guide
â”‚   â”‚   â””â”€â”€ ğŸ“˜ troubleshooting.md        # Troubleshooting guide
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ development/                  # Developer documentation
â”‚       â”œâ”€â”€ ğŸ”§ setup.md                  # Development setup
â”‚       â”œâ”€â”€ ğŸ”§ contributing.md           # Contribution guidelines
â”‚       â””â”€â”€ ğŸ”§ architecture.md           # System architecture
â”‚
â”œâ”€â”€ ğŸ“ assets/                           # Static assets
â”‚   â”œâ”€â”€ ğŸ“ images/                       # Images and screenshots
â”‚   â”‚   â”œâ”€â”€ ğŸ–¼ï¸ logo.png                  # Application logo
â”‚   â”‚   â”œâ”€â”€ ğŸ–¼ï¸ demo_screenshot.png       # Demo screenshots
â”‚   â”‚   â””â”€â”€ ğŸ–¼ï¸ gesture_examples.png      # Gesture examples
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ videos/                       # Demo videos
â”‚   â”‚   â”œâ”€â”€ ğŸ¥ demo_video.mp4            # Application demo
â”‚   â”‚   â””â”€â”€ ğŸ¥ gesture_tutorial.mp4      # Gesture tutorial
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ audio/                        # Audio files
â”‚   â”‚   â”œâ”€â”€ ğŸ”Š notification.wav          # UI notifications
â”‚   â”‚   â””â”€â”€ ğŸ”Š voice_samples.wav         # Voice command samples
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ icons/                        # UI icons
â”‚   â”œâ”€â”€ ğŸ“ fonts/                        # Custom fonts
â”‚   â””â”€â”€ ğŸ“ sample_data/                  # Sample datasets
â”‚
â”œâ”€â”€ ğŸ“ logs/                             # Application logs
â”‚   â”œâ”€â”€ ğŸ“Š app.log                       # Main application log
â”‚   â”œâ”€â”€ ğŸ–ï¸ gesture.log                   # Gesture detection log
â”‚   â”œâ”€â”€ ğŸ¤ voice.log                     # Voice recognition log
â”‚   â””â”€â”€ ğŸ¤– ai.log                        # AI assistant log
â”‚
â””â”€â”€ ğŸ“ notebooks/                        # Jupyter notebooks
    â”œâ”€â”€ ğŸ““ data_exploration.ipynb         # Data exploration examples
    â”œâ”€â”€ ğŸ““ gesture_analysis.ipynb         # Gesture detection analysis
    â”œâ”€â”€ ğŸ““ voice_testing.ipynb            # Voice recognition testing
    â””â”€â”€ ğŸ““ ai_prompt_testing.ipynb        # AI prompt engineering
```

## ğŸš€ Quick Start Guide

### 1. Installation
```bash
# Clone the repository
git clone <repository-url>
cd dataisart

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.template .env
# Edit .env with your OpenAI API key
```

### 2. Configuration
```bash
# Set your OpenAI API key
export OPENAI_API_KEY="your-api-key-here"

# Optional: Set other environment variables
export DEBUG=True
export LOG_LEVEL=INFO
```

### 3. Run the Application
```bash
# Start the Streamlit application
streamlit run main.py

# Or run directly with Python
python main.py
```

## ğŸ“‹ Core Components

### ğŸ–ï¸ Hand Tracking (`src/core/hand_tracking/`)
- **Purpose**: Detect and interpret hand gestures using MediaPipe
- **Key Features**:
  - Real-time hand landmark detection
  - Gesture classification (pointing, selection, etc.)
  - Data region selection through hand movements
  - Integration with dataset viewer

### ğŸ¤ Voice Recognition (`src/core/voice_recognition/`)
- **Purpose**: Convert speech to text and parse commands
- **Key Features**:
  - Multiple speech recognition engines (Google, Whisper)
  - Command intent classification
  - Continuous listening mode
  - Voice command validation

### ğŸ¤– LLM Integration (`src/core/llm_integration/`)
- **Purpose**: AI-powered data analysis and visualization
- **Key Features**:
  - OpenAI GPT-4 integration
  - Context-aware prompts with dataset information
  - Structured response parsing
  - Conversation history management

### ğŸ“Š Data Processing (`src/core/data_processing/`)
- **Purpose**: Dataset management and analysis
- **Key Features**:
  - Multiple format support (CSV, Excel, JSON, Parquet)
  - Statistical analysis and summaries
  - Data selection and filtering
  - Memory-efficient processing

### ğŸ“ˆ Visualization (`src/core/visualization/`)
- **Purpose**: Generate interactive charts and plots
- **Key Features**:
  - Plotly-based interactive visualizations
  - Multiple chart types (histogram, scatter, bar, line)
  - Dynamic chart generation from voice commands
  - Customizable styling and themes

## ğŸ¯ Usage Examples

### Voice Commands
```
"Show me the age distribution"
"Visualize the correlation between age and income"
"Create a scatter plot of height vs weight"
"Analyze the missing values in the dataset"
"Filter the data where age is greater than 30"
```

### Hand Gestures
- **Pointing**: Select individual data points or columns
- **Box Selection**: Draw selection boxes around data regions
- **Swipe**: Navigate through dataset pages
- **Pinch**: Zoom in/out of visualizations

## ğŸ”§ Development Guidelines

### Adding New Features
1. **Core Modules**: Add new functionality to `src/core/`
2. **UI Components**: Create reusable components in `src/ui/components/`
3. **Tests**: Write tests in appropriate `tests/` subdirectories
4. **Documentation**: Update relevant documentation in `docs/`

### Code Organization
- Follow the existing directory structure
- Use type hints for better code clarity
- Include docstrings for all public methods
- Follow PEP 8 style guidelines

### Testing
```bash
# Run unit tests
python -m pytest tests/unit/

# Run integration tests
python -m pytest tests/integration/

# Run all tests
python -m pytest tests/
```

## ğŸ“ Configuration Management

### Environment Variables
- `OPENAI_API_KEY`: Required for AI functionality
- `DEBUG`: Enable debug mode
- `LOG_LEVEL`: Set logging level (DEBUG, INFO, WARNING, ERROR)
- `CAMERA_DEVICE_ID`: Camera device identifier
- `VOICE_RECOGNITION_TIMEOUT`: Voice recognition timeout

### Application Settings
- Modify `config/app/settings.py` for application-wide settings
- Adjust `config/models/` for model-specific configurations
- Update `config/api/` for API-related settings

## ğŸš¨ Troubleshooting

### Common Issues
1. **Camera not working**: Check camera permissions and device ID
2. **Voice recognition failing**: Verify microphone permissions and audio levels
3. **AI not responding**: Ensure OpenAI API key is correctly set
4. **Import errors**: Check that all dependencies are installed

### Debug Mode
Enable debug mode by setting `DEBUG=True` in your environment variables for detailed logging and error messages.

## ğŸ“š Additional Resources

- **User Guide**: `docs/user_guide/getting_started.md`
- **API Documentation**: `docs/api/core_modules.md`
- **Development Setup**: `docs/development/setup.md`
- **Contributing Guidelines**: `docs/development/contributing.md`

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Update documentation
6. Submit a pull request

---

*This project structure is designed to be modular, scalable, and maintainable. Each component has a specific responsibility, making it easy to extend and modify the application.* 